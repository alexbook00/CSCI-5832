{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "strong-elder",
   "metadata": {},
   "source": [
    "# CSCI 5832 - Natural Language Processing\n",
    "# Assignment 2: Logistic Regression and Sentiment Analysis\n",
    "#### Author: Alex Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "supposed-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import csv\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-touch",
   "metadata": {},
   "source": [
    "##### Filling arrays of positive and negative words from $\\textbf{positive-words.txt}$ and $\\textbf{negative-words.txt}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "consolidated-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill array of positive words\n",
    "pos_words = []\n",
    "with open('positive-words.txt') as f:\n",
    "    for line in f:\n",
    "        pos_words.append(line.split()[0])\n",
    "        \n",
    "# fill array of negative words\n",
    "neg_words = []\n",
    "with open('negative-words.txt') as f:\n",
    "    for line in f:\n",
    "        neg_words.append(line.split()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-dallas",
   "metadata": {},
   "source": [
    "##### Defining function to extract features from reviews (turn reviews into vectors) and turning $\\textbf{hotelPosT-train.txt}$ and $\\textbf{hotelNegT-train.txt}$ into a list of vectors held in a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "automotive-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pronouns array\n",
    "pronouns = ['i', 'me', 'mine', 'my', 'you', 'your', 'yours', 'we', 'us', 'ours']\n",
    "\n",
    "def get_features_from_reviews(file):\n",
    "    with open(file, encoding=\"utf8\") as f:\n",
    "        # initialize array of id_features_class vectors\n",
    "        id_features_class_vectors = []\n",
    "        \n",
    "        for line in f:\n",
    "            # initialize features vector\n",
    "            id_features_class = ['ID', 0, 0, 0, 0, 0, 0, 'Class']\n",
    "            \n",
    "            # getting \n",
    "            if 'Pos' in file:\n",
    "                id_features_class[-1] = 1\n",
    "            elif 'Neg' in file:\n",
    "                id_features_class[-1] = 0\n",
    "            else:\n",
    "                id_features_class[-1] = 999\n",
    "\n",
    "            # check for feature 3\n",
    "            if 'no' in line.lower():\n",
    "                id_features_class[3] = 1\n",
    "\n",
    "            # check for feature 5\n",
    "            if '!' in line:\n",
    "                id_features_class[5] = 1\n",
    "\n",
    "            # split review by spaces\n",
    "            arr = line.split()\n",
    "            \n",
    "            # get ID\n",
    "            id_features_class[0] = arr[0]\n",
    "\n",
    "            # get feature 6 (length minus one due to the ID being the first 'word')\n",
    "            id_features_class[6] = np.log(len(arr)-1)\n",
    "\n",
    "            # get remaining features\n",
    "            for word in arr[1:]:\n",
    "                # put word to lowercase and remove any punctuation at end of word\n",
    "                word = word.lower()\n",
    "                if word[-1] in string.punctuation:\n",
    "                    word = word[:-1]\n",
    "                \n",
    "                # check for contribution to feature 1\n",
    "                if word in pos_words:\n",
    "                    id_features_class[1] += 1\n",
    "                    \n",
    "                # check for contribution to feature 2\n",
    "                if word in neg_words:\n",
    "                    id_features_class[2] += 1\n",
    "                    \n",
    "                # check for contribution to feature 4\n",
    "                if word in pronouns:\n",
    "                    id_features_class[4] += 1\n",
    "            \n",
    "            # append features vector to array holding all of them\n",
    "            id_features_class_vectors.append(id_features_class)\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "        return id_features_class_vectors\n",
    "\n",
    "all_features_vectors = get_features_from_reviews('hotelPosT-train.txt') + get_features_from_reviews('hotelNegT-train.txt')\n",
    "\n",
    "# output to csv\n",
    "with open('Book-Alex-assgn2-part1.csv', 'w', newline='') as outfile:\n",
    "    csvWriter = csv.writer(outfile, delimiter=',')\n",
    "    csvWriter.writerows(all_features_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-greensboro",
   "metadata": {},
   "source": [
    "##### Turning .csv file back into vectors and splitting the data up into training and development sets. Also keeping the whole set together for final testing before evaluating the provided test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "empty-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into an array\n",
    "with open('Book-Alex-assgn2-part1.csv', newline='') as infile:\n",
    "    data = list(csv.reader(infile))\n",
    "    \n",
    "    infile.close()\n",
    "\n",
    "# shuffle data array\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# split data into training and development\n",
    "training_data = data[:int(len(data)*.8)]\n",
    "development_data = data[int(len(data)*.8):]\n",
    "\n",
    "# split data and development data into (ID, features, label) 3-tuples\n",
    "ids_features_labels_training = [[x[0], np.array([float(num) for num in x[1:-1]]), int(x[-1])] for x in training_data]\n",
    "ids_features_labels_development = [[x[0], np.array([float(num) for num in x[1:-1]]), int(x[-1])] for x in development_data]\n",
    "ids_features_labels_total = [[x[0], np.array([float(num) for num in x[1:-1]]), int(x[-1])] for x in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-worth",
   "metadata": {},
   "source": [
    "##### Defining Stochastic Gradient Descent function, as well as the loss function (Cross Entropy) and estimated output function (sigmoid function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "offensive-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(L, f, x_y):\n",
    "    # L is the loss function (in this case we use Cross Entropy)\n",
    "    # f is our estimated output function (use the sigmoid function: 1/(1 + e^-(w dot x + b)) where w is the weights, x is the features, and b is the bias term)\n",
    "    # x is the set of training inputs (feature vectors)\n",
    "    # y is the set of training output (labels)\n",
    "    \n",
    "    # initialize weights and bias\n",
    "    weights = np.zeros(6)\n",
    "    bias = 1\n",
    "    \n",
    "    learning_rate = .01\n",
    "    \n",
    "    overall_losses = []\n",
    "    \n",
    "    for _ in range(1000):\n",
    "        np.random.shuffle(x_y)\n",
    "        # for each training tuple (x, y) (in random order):\n",
    "        losses = []\n",
    "        for i in range(len(x_y)):\n",
    "            features = x_y[i][1]\n",
    "            y = x_y[i][2] # this is the label/correct score\n",
    "\n",
    "            # compute our estimated output\n",
    "            rawscore = np.dot(weights, features) + bias # w dot x plus b\n",
    "            y_hat = f(rawscore) # this is the computed score\n",
    "            \n",
    "            # compute loss for bookkeeping\n",
    "            loss = L(y, y_hat)\n",
    "            losses.append(loss)\n",
    "\n",
    "            # compute the gradient weights\n",
    "            gradient = (y_hat - y) * features # this is delta\n",
    "\n",
    "            # how should we move the set of weights to maximize loss?\n",
    "            # go the other way instead\n",
    "            weights = weights - (learning_rate * gradient)\n",
    "        \n",
    "        overall_losses.append(np.sum(losses))\n",
    "        \n",
    "    # return set of weights\n",
    "    return weights\n",
    "\n",
    "def cross_entropy(y, y_hat):\n",
    "    return -(y * np.log(y_hat) + (1-y) * np.log(1 - y_hat))\n",
    "\n",
    "def sigmoid(score):\n",
    "    return 1/(1 + np.exp(-score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-final",
   "metadata": {},
   "source": [
    "##### Testing the accuracy of the program by averaging the results of ten different sets of learned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "pregnant-dress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "def find_accuracy(weights, ids_features_labels):\n",
    "    num_correct = 0\n",
    "    \n",
    "    for i in range(len(ids_features_labels)):\n",
    "        features = ids_features_labels[i][1]\n",
    "        label = ids_features_labels[i][2]\n",
    "        \n",
    "        estimate = sigmoid(np.dot(weights, features) + 1)\n",
    "        if (estimate > .5 and label == 1) or (estimate < .5 and label == 0):\n",
    "            num_correct += 1\n",
    "        \n",
    "    return num_correct/len(ids_features_labels)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    weights = SGD(cross_entropy, sigmoid, ids_features_labels_total)\n",
    "    accuracies.append(find_accuracy(weights, ids_features_labels_total))\n",
    "\n",
    "print(np.average(np.array(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-andrews",
   "metadata": {},
   "source": [
    "##### Classifying the given test set using a single set of learned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "failing-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_vectors_test = get_features_from_reviews('HW2-testset.txt')\n",
    "ids_features_labels_test = [[x[0], np.array([float(num) for num in x[1:-1]]), int(x[-1])] for x in features_vectors_test]\n",
    "\n",
    "weights = SGD(cross_entropy, sigmoid, ids_features_labels_total)\n",
    "\n",
    "for i in range(len(ids_features_labels_test)):\n",
    "    features = ids_features_labels_test[i][1]\n",
    "    \n",
    "    estimate = sigmoid(np.dot(weights, features) + 1)\n",
    "    if estimate > .5:\n",
    "        ids_features_labels_test[i][2] = 'POS'\n",
    "    else:\n",
    "        ids_features_labels_test[i][2] = 'NEG'\n",
    "        \n",
    "with open('Book-Alex-assgn2-out.txt', 'w') as f:\n",
    "    for item in ids_features_labels_test:\n",
    "        f.write('{} \\t {} \\n'.format(item[0], item[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
